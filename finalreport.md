# KDDcup 最终报告 

计52 沈俊贤 2015011258

计52 王纪霆 2015011251

## 小组成员分工与工作量 

#### 沈俊贤

完成了数据预处理、xgboost模型构建、生成xgboost版本的submission部分

#### 王纪霆

完成了数据集生成，基于tensorflow的mlp模型的构建、训练与预测。

## 数据预处理

- 在对原始数据进行下载之后，我们首先通过数据分类，将全年所有站点的数据分割成每个站点一个文件，再对每个文件进行相应的处理。
- 在对缺失数据进行处理的过程中，我们对连续缺失量较大的数据，采取填入过去一段时间的平均值的方法；对连续缺失量较小的数据，填入上一个时间内的对应数据。
- 在处理的过程中，考虑到直接将每小时的数据送入模型，可能会产生较大的波动，并且数据越久远，对当前的空气质量贡献越小，我们将“过去某一段时间的平均值”进行计算，更好地提取出了“时序”特征，通过对“过去一个月”、“过去两周”、“过去一周”、“过去三天”、“过去一天”、“过去12小时”、“过去6小时 ”、“过去3小时”这些特征的计算，增强了对时序的利用。
- 在计算平均值的时候，我们采用了简单的算术平均值的方法。在计算风速的时候，考虑到风速和风向两个因素实际上同时决定了“风”这个向量，所以我们在计算风的时候，将风速和风向两个因素都考虑了进去，作为一个向量，进行向量的算术平均数计算，最后还原成风速和风向。

## 模型构建中所作的尝试 

### MLP模型

我们先后使用了两种不同的MLP模型，第一种仅使用过去一段时间的天气情况预测当前空气质量（当然，由于需要预测将来48小时的空气质量情况，因此这一段时间的天气只能通过天气预报得到），另一种则考虑了过去一段时间的空气质量对当前空气质量的影响。以下分别阐述两种模型的结构。

#### 模型一

这一模型使用格点的天气数据预测空气质量。由于天气数据产生于各个气象站点，而空气质量数据则只产生于空气质量监测站，两者在数量、位置上都有所不同，并且在北京和伦敦两个城市之间也有差异。虽然只采用距空气质量站最近的几个格点的数据也可以，但我们希望使用到所有格点的数据，这样可以尽可能地避免异常值的干扰。

根据常理，我们认为距离越远的格点对空气质量的影响就越小，但其影响和距离之间的关系不易确定。为将这一空间关系加入模型当中，我们使用了一个三维距离矩阵$D$来表述站点间的距离关系。

对相隔距离为$d$的空气质量站点$i$和气象站点$j$，我们用一个由经验而来的向量$[d, d^{-1}, d^{-1.5}, d^{-2}, d^{-3}, d^{-4} ]$来作为其距离特征。将这些距离特征拼接在一起，即得到了$n \times m \times 6$的距离矩阵，其中$n, m$分别表示空气质量站点数和气象站点数。

这个距离矩阵按照最后一个维度被归一化，以避免d的不同次幂导致的数量级差异。

设$d_n, d_m$分别表示经预处理之后的空气质量数据、气象数据的数据维数。则模型的输入是$m \times d_m$的气象数据矩阵，输出为$n \times d_n$的空气质量数据矩阵。

每层的参数如下:

Layer 1: $m \times d_m \to m \times 128$, Batch Normalization + 0.7 dropout

Layer 2: $m \times 128 \to m \times 128 \times 6$

Layer 3: 与距离矩阵$D$相乘，$128 \times m \times 6 \to 128 \times n$

Layer 4: $n \times 128 \to n \times 128$, Batch Normalization + 0.7 dropout

Layer 5: $n \times 128 \to n \times d_n$

以上即全网络的结构。

#### 模型二

在模型一的基础上，添加了对过去空气质量数据的使用。虽然用上了更多的数据，但使用这一模型时，只能一小时一小时地迭代进行预测，并将上一小时的预测结果作为输入预测下一小时的空气质量数据，这可能导致预测的误差在一次次预测中被放大。

令$d_p$表示历史空气质量数据的总维度。模型的输入是$m \times d_m$的气象数据矩阵以及$n\times d_p$的历史空气质量数据矩阵，输出为$n \times d_n$的空气质量数据矩阵。

历史空气质量数据经过以下几层：

Layer 1': $n \times d_n \to n \times 256$, Batch Normalization + 0.7 dropout

Layer 2': $n \times 256 \to n \times 128$, Batch Normalization + 0.7 dropout

将模型一中的Layer 4与上述的Layer 2'相拼合，形成$n \times 256$的矩阵，这样便结合了空气质量、气象的数据，再经过一个线性层得到结果。

#### 训练

两模型均使用预测结构与实际值的squared difference作为Loss。learning rate设为0.001，并每个epoch以0.95的因子指数下降。使用给出的过去一年内数据作为训练数据，取前80%为训练集，后20%为验证集，以4月中数据为测试集。

经过大约30个epoch左右的训练，在验证集、测试集上SMAPE达到0.4-0.5左右的水平，就以这一结果进行了预测。另外，还发现对北京而言，模型二效果更好，对伦敦而言，模型一效果更好，但具体原因并不清楚。

### 决策树相关

因为xgboost作为决策树的工具包，既满足了我们想要使用决策树对数据进行特征提取的想法，也满足了对大量数据加速处理从而不至于让训练时间过慢，从而影响参数的调试。

在使用xgboost的时候 ，通过对xgboost的`eta`参数和`maxdepth`参数进行有间隔遍历，测试的时候使用了5月6日的数据，最后得到在`eta`为`0.4`，`maxdepth`为`10`的时候效果最好。

### 加速决策 

我们在实践中发现，对某一个特定的空气监测站站点来说，最具有代表性的气象数据可以局限在距离它最近的几个站点的数据中，一方面这几个数据最具有代表性；另外一方面距离它较远的数据失去了和它的数据相关性。所以我们在进行决策的时候，对于空气监测站来说，只选取了离它最近的四个气象监测站的站点数据，既保证了最后预测的准确性，同时也保证了决策在较短的时间内完成训练和预测。

## 使用的工具

- xgboost
- tensorflow
- scikit-learn

## 最终采用的方案

我们采用的一个方案是xgboost，取得的总体效果比较稳定，但是效果并不算好(SMAPE=0.5-0.7)，我们猜测决策树因为本身原理的限制，会陷入总体效果稳定但是不好和个别效果好但是过拟合的困境。

另一个采用的方案为MLP，在初始取得了相对较好的效果(SMAPE=0.44)，但之后随着日期变化，可能由于空气质量的分布整体发生了变化，模型的预测效果也渐渐变差，最终停滞在0.5左右。

##  代码的组织结构

考虑到我们写的功能没有那么复杂，所有功能都直接以脚本的形式呈现，这样开发和使用时都比较直观。

### 数据预处理

所有数据文件均被放在data文件夹下，预处理之后的结果也将产生在这里。

+ datainit.py：用于进行简单的预处理

+ dataparser.py：用于进行数据预处理

### 数据获取与提交

+ dataset.py：为数据集模块，提供了以numpy array格式获取batch data的接口。

+ pred\_dataset.py：同样是数据集模块，但仅预测时使用。

+ grabdata.py：用于通过api获取下一天预测所需要的数据，如`python grabdata.py 2018-05-10`为获取5月10日的新数据，包括11,12日的天气预报数据，10日的天气、空气质量数据。

+ api\_submit.py：官方提供的api提交文件。

+ private\_config.py：包含队伍token等不便于放在github上的信息

### MLP模型

+ mlp\_main.py ：MLP模型一的入口文件，--test参数可以指定为训练还是预测，--name指定模型名，--date在预测时指定预测的日期，--inf指定模型的版本（默认为最新版本），--city指定模型对应的城市。

+ mlp\_model.py：为MLP模型一的具体实现。

+ aq\_mlp\_main.py：MLP模型二的入口文件，使用方式与模型一相同。

+ aq\_mlp\_model.py：为MLP模型二的具体实现。

+ sequenced\_data.py：用于实现模型二预测中的迭代预测功能（不断产生下一个预测数据、并用生成结果再次作为输入数据）

### xgboost模型

+ xgb\_model.py：定义xgboost模型需要的数据。

+ xgb\_main.py：具体实现xgboost的训练、预测等等